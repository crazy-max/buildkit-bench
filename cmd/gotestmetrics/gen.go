package main

import (
	"encoding/json"
	"log"
	"math"
	"os"
	"path/filepath"

	"github.com/go-echarts/go-echarts/v2/charts"
	"github.com/go-echarts/go-echarts/v2/components"
	"github.com/go-echarts/go-echarts/v2/opts"
	"github.com/moby/buildkit-bench/util/candidates"
	"github.com/moby/buildkit-bench/util/gotest"
	"github.com/moby/buildkit-bench/util/testutil"
	"github.com/pkg/errors"
)

type genCmd struct {
	Files []string `kong:"arg='',name='files',required,help='Benchmark results files generated by parse command.'"`

	Config     string `kong:"name='config',required,default='testconfig.yml',help='Test config file.'"`
	Output     string `kong:"name='output',default='./bin/benchmarks.html',help='File to write the HTML report to.'"`
	Candidates string `kong:"name='candidates',help='Candidates file.'"`
}

func (c *genCmd) Run(ctx *Context) error {
	benchmarks, err := gotest.MergeBenchmarks(c.Files)
	if err != nil {
		return err
	}
	if ctx.Debug {
		b, _ := json.MarshalIndent(benchmarks, "", "  ")
		log.Printf("%s", string(b))
	}
	if err := c.validateBenchmarks(benchmarks); err != nil {
		return err
	}
	if err := os.MkdirAll(filepath.Dir(c.Output), 0755); err != nil {
		return errors.Wrap(err, "failed to create output directory")
	}
	return c.writeHTML(benchmarks)
}

func (c *genCmd) validateBenchmarks(benchmarks map[string]gotest.Benchmark) error {
	log.Printf("Validating %d benchmark results based on %s", len(benchmarks), c.Config)
	tc, err := testutil.LoadTestConfig(c.Config)
	if err != nil {
		return err
	}
	seen := make(map[string]struct{})
	for _, benchmark := range benchmarks {
		if _, ok := seen[benchmark.Name]; !ok {
			seen[benchmark.Name] = struct{}{}
		}
		bm, err := tc.BenchmarkConfig(benchmark.Name)
		if err != nil {
			return err
		}
		for _, run := range benchmark.Runs {
			for unit := range run.Extra {
				if _, ok := bm.Metrics[unit]; !ok {
					return errors.Errorf("unknown metric %q for benchmark %q", unit, benchmark.Name)
				}
			}
		}
	}
	for rootName, bms := range tc.Runs {
		for testName := range bms {
			if _, ok := seen[rootName+"/"+testName]; !ok {
				return errors.Errorf("missing benchmark result for %q", rootName+"/"+testName)
			}
		}
	}
	return nil
}

func (c *genCmd) writeHTML(benchmarks map[string]gotest.Benchmark) error {
	log.Printf("Generating HTML report to %s", c.Output)
	benchmarksRuns := make(map[string][]gotest.BenchmarkRun)
	for _, benchmark := range benchmarks {
		br, ok := benchmarksRuns[benchmark.Name]
		if ok {
			benchmarksRuns[benchmark.Name] = append(br, benchmark.Runs...)
		} else {
			benchmarksRuns[benchmark.Name] = benchmark.Runs
		}
	}

	tc, err := testutil.LoadTestConfig(c.Config)
	if err != nil {
		return err
	}

	var sortedRefs []candidates.Ref
	if c.Candidates != "" {
		cds, err := candidates.Load(c.Candidates)
		if err != nil {
			return errors.Wrapf(err, "failed to load candidates from %s", c.Candidates)
		}
		sortedRefs = cds.Sorted()
	}

	var cps []components.Charter

	for name, runs := range benchmarksRuns {
		bc, err := tc.BenchmarkConfig(name)
		if err != nil {
			return err
		}

		metrics := make(map[string]map[string][]float64)
		for _, run := range runs {
			for unit := range bc.Metrics {
				if _, ok := metrics[unit]; !ok {
					metrics[unit] = make(map[string][]float64)
				}
				if v, ok := run.Extra[unit]; ok {
					metrics[unit][run.Ref] = append(metrics[unit][run.Ref], v)
				} else {
					return errors.Errorf("missing metric %q for run %s", unit, run.Ref)
				}
			}
		}

		for unit, values := range metrics {
			var refs []string
			var data []opts.BarData
			var totalValue float64
			if len(sortedRefs) == 0 {
				for ref, value := range values {
					avgv := 0.0
					for _, v := range value {
						avgv += v
					}
					avgv /= float64(len(value))
					totalValue += avgv
					refs = append(refs, ref)
					data = append(data, opts.BarData{Value: math.Round(avgv*100000) / 100000})
				}
			} else {
				for _, ref := range sortedRefs {
					value, ok := values[ref.Name]
					if !ok {
						return errors.Errorf("%s missing %s value for ref %s", name, unit, ref.Name)
					}
					avgv := 0.0
					for _, v := range value {
						avgv += v
					}
					avgv /= float64(len(value))
					totalValue += avgv
					refs = append(refs, ref.Name)
					data = append(data, opts.BarData{Value: math.Round(avgv*100000) / 100000})
				}
			}

			averageValue := totalValue / float64(len(refs))
			averageData := make([]opts.LineData, len(refs))
			for i := 0; i < len(refs); i++ {
				averageData[i] = opts.LineData{Value: averageValue}
			}

			chart := charts.NewBar() // TODO: chart type should be inferred from test config
			averageLine := charts.NewLine()
			globalOptions := []charts.GlobalOpts{
				charts.WithTitleOpts(opts.Title{
					Title:    bc.Description,
					Subtitle: name,
				}),
			}
			if len(refs) > 10 {
				globalOptions = append(globalOptions, charts.WithDataZoomOpts(opts.DataZoom{
					Type:  "slider",
					Start: 70,
				}))
			}
			chart.SetGlobalOptions(globalOptions...)
			chart.SetXAxis(refs).AddSeries(bc.Metrics[unit].Description, data)
			averageLine.SetXAxis(refs).AddSeries("Average", averageData)
			chart.Overlap(averageLine)
			cps = append(cps, chart)
		}
	}

	page := components.NewPage()
	page.PageTitle = "BuildKit Benchmarks"
	page.Layout = components.PageFlexLayout
	page.AddCharts(cps...)

	f, err := os.Create(c.Output)
	if err != nil {
		return errors.Wrap(err, "failed to create file")
	}
	defer f.Close()

	if err := page.Render(f); err != nil {
		return errors.Wrap(err, "failed to render page")
	}

	return nil
}
